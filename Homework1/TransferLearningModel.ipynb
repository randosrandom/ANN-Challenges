{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Transfer_model.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1OtIP58drr0g"},"source":["# Introductive code"]},{"cell_type":"markdown","metadata":{"id":"6KCQcosCnumd"},"source":["In order to work properly, our notebook requires 2 folders:\n","\n","+ \"MaskDataset\" folder, which is the result of unzipping 'artificial-neural-networks-and-deep-learning-2020.zip': we've created a cell that unzips the file, but it should be run only if \"MaskDataset\" folder wasn't already created\n","\n","+ \"MaskDatasetSorted\" folder, which contains three folders (one per label) with all the images for flow_from_directory: it can be created running the three code cells below the \"Sort the images for flow_from_directory method\" title, and each of them should be run only 1 time (in order to avoid multiple copies of the same image)"]},{"cell_type":"code","metadata":{"id":"jR_EDtobrCo2"},"source":["# Print all the intermediate operations (for debugging)\n","\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Plj85XWYr3J5"},"source":["# Import most relevant libraries\n","\n","import os\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","# Import library for handling json files\n","\n","import json\n","\n","\n","# Import shutil for the copy in sorted folders\n","\n","import shutil"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkBfCKBPr9KT"},"source":["# Fix a seed\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zmii4hOHrgRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605941441042,"user_tz":-60,"elapsed":19530,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"9329a5b3-ad77-4f3b-9225-9a1806ef456e"},"source":["# Add Colab (with Drive)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKTUEVYivWo2"},"source":["# If the \"MaskDataset\" folder wasn't already created, this cell unzips the zip file and creates \"MaskDataset\";\n","# please run this cell only if \"MaskDataset\" wasn't already created\n","\n","!unzip '/content/drive/My Drive/ANNDL_Results/Challenge1/artificial-neural-networks-and-deep-learning-2020.zip' -d '/content/drive/My Drive/ANNDL_Results/Challenge1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FibacQUT5-Ro"},"source":["# Path to the current working directory (which contains MaskDataset folder)\n","\n","cwd = '/content/drive/My Drive/ANNDL_Results/Challenge1' # Working directory, with the dataset folder (we had issues with os.getcwd())\n","\n","# The following variables are inherited from previous code for Kaggle: they may be \n","# changed to switch to Kaggle without changing the rest of the implementation\n","\n","work_dir = cwd\n","work_dir2 = cwd\n","work_dir3 = cwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ml0kjpqIuRrA"},"source":["# Path to the dataset folder (MaskDataset folder must be already created)\n","\n","dataset_dir = os.path.join(work_dir,\"MaskDataset\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRXza3vKuVNn"},"source":["# Sort the images for flow_from_directory method"]},{"cell_type":"markdown","metadata":{"id":"QO5BPlMh5-Rx"},"source":["Run the following 3 cells to create \"MaskDatasetSorted\" folder, which contains three folders (one per label) with all the images for flow_from_directory. Please run these 3 cells only if \"MaskDatasetSorted\" wasn't already created"]},{"cell_type":"code","metadata":{"id":"K3KexZ8kudeZ"},"source":["# From the file .json, extract a dictionary with:\n","\n","# Key: name of the image\n","# Value: label of the image\n","\n","with open(os.path.join(dataset_dir,\"train_gt.json\")) as json_file: \n","    labels = json.load(json_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0VS3V58wfEX"},"source":["# Create the folders for flow_from_directory method\n","\n","sorted_training_dir = os.path.join(work_dir2,\"MaskDatasetSorted\")\n","if not os.path.exists(sorted_training_dir):\n","    os.makedirs(sorted_training_dir)\n","\n","folder_0 = os.path.join(sorted_training_dir,\"label0\")\n","if not os.path.exists(folder_0):\n","    os.makedirs(folder_0)\n","    \n","folder_1 = os.path.join(sorted_training_dir,\"label1\")\n","if not os.path.exists(folder_1):\n","    os.makedirs(folder_1)\n","\n","folder_2 = os.path.join(sorted_training_dir,\"label2\")\n","if not os.path.exists(folder_2):\n","    os.makedirs(folder_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaQN01t15-R1"},"source":["# Copy the images in the proper folder\n","\n","training_dir = os.path.join(dataset_dir, 'training')\n","\n","for name in labels.keys():\n","    photo_dir = os.path.join(training_dir,name)\n","    \n","    if labels[name] == 0:\n","        shutil.copy2(photo_dir,folder_0)\n","    if labels[name] == 1:\n","        shutil.copy2(photo_dir,folder_1)\n","    if labels[name] == 2:\n","        shutil.copy2(photo_dir,folder_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"agmpcp6Us4PT"},"source":["# Set all the necessary variables in ImageDataGenerator"]},{"cell_type":"code","metadata":{"id":"N7PemM47tT6F"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input \n","\n","apply_data_augmentation = True\n","\n","# Create training ImageDataGenerator object (for data augmentation and for transfer learning)\n","\n","if apply_data_augmentation:\n","    train_data_gen = ImageDataGenerator(rotation_range=10,\n","                                        width_shift_range=7,\n","                                        height_shift_range=7,\n","                                        zoom_range=0.15,\n","                                        horizontal_flip=True,\n","                                        vertical_flip=False,\n","                                        fill_mode='constant',\n","                                        cval=0,\n","                                        validation_split = 0.08,\n","                                        preprocessing_function=preprocess_input)  # to apply vgg normalization\n","else:\n","    train_data_gen = ImageDataGenerator(validation_split = 0.08,\n","                                        preprocessing_function=preprocess_input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gr6h5KKJuc7-"},"source":["# Create train and validation generators"]},{"cell_type":"code","metadata":{"id":"ddfwk1B55-R4"},"source":["# Path to the training folder (data will be splitted, using ImageDataGenerator objects with flow_from_directory method)\n","\n","training_dir = os.path.join(work_dir2, 'MaskDatasetSorted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBeYpOXDurKo"},"source":["# Some useful parameters\n","\n","# Batch size\n","bs = 32\n","\n","# img shape\n","img_h = 360\n","img_w = 360\n","\n","# Number of classes\n","num_classes = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJI6LRR01TWj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605941474596,"user_tz":-60,"elapsed":27435,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"f4c46b51-8f64-458d-e93f-634012114554"},"source":["# Create generators to read images from MaskDatasetSorted (created before)\n","\n","decide_class_indices = True\n","if decide_class_indices:\n","    classes = ['label0',    # 0\n","               'label1',    # 1\n","               'label2']    # 2\n","else:\n","    classes=None\n","\n","# Training\n","    \n","train_gen = train_data_gen.flow_from_directory(training_dir,\n","                                               target_size=(img_h, img_w),\n","                                               color_mode=\"rgb\",\n","                                               classes=classes,\n","                                               class_mode='categorical',\n","                                               batch_size=bs,\n","                                               shuffle=True,\n","                                               seed=SEED,\n","                                               subset='training')\n","\n","# Validation\n","\n","valid_gen = train_data_gen.flow_from_directory(training_dir,\n","                                               target_size=(img_h, img_w),\n","                                               color_mode=\"rgb\",\n","                                               classes=classes,\n","                                               class_mode='categorical',\n","                                               batch_size=bs,\n","                                               shuffle=True,\n","                                               seed=SEED,\n","                                               subset='validation')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 5166 images belonging to 3 classes.\n","Found 448 images belonging to 3 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oKwBDHQZ5-SB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605941474597,"user_tz":-60,"elapsed":25697,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"6ca6aeee-0ad9-4e53-c32c-6ac0a5a0ab3e"},"source":["# Check how keras assigned the labels\n","train_gen.class_indices"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label0': 0, 'label1': 1, 'label2': 2}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"itn22HFJitsp"},"source":["# Create Dataset objects both for training and for validation with tf.data.Dataset.from_generator\n","\n","\n","# Training\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n","train_dataset = train_dataset.repeat()\n","\n","\n","# Validation\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5GALFD_qjLky"},"source":["# Build and fit the model"]},{"cell_type":"code","metadata":{"id":"8P3_qk2NWLbX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605941480946,"user_tz":-60,"elapsed":28243,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"73738953-5c80-4f35-bff8-fbaedb55e83e"},"source":["# Load VGG16 Model\n","\n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n","\n","# Create Model\n","\n","model = tf.keras.Sequential()\n","\n","# Create the convolutional network for transfer learning (or fine tuning)\n","\n","finetuning = True\n","\n","if finetuning:\n","    freeze_until = 15 # layer from which we want to fine-tune\n","    \n","    for layer in vgg.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    vgg.trainable = False\n","\n","model.add(vgg)\n","\n","# Set the fully connected layers for classification\n","\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dropout(0.5))\n","model.add(tf.keras.layers.Dense(units=224, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n","model.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n","model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qm5nKNWSj6uv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605941480948,"user_tz":-60,"elapsed":26666,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"5b842f0b-e2ac-44d8-c1db-f84e00cd0dc1"},"source":["# Visualize created model as a table\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 11, 11, 512)       14714688  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 61952)             0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 61952)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 224)               13877472  \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                7200      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 28,599,459\n","Trainable params: 20,964,195\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GoWnZJ3ckHXE"},"source":["# Optimization params\n","\n","# Loss\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","\n","# Learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","\n","# Validation metrics\n","metrics = ['accuracy']\n","\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68i_M9VV4Q1n"},"source":["We performed a training of 25 epochs on the model. Differently from our best transfer learning - based network, training seems to start also in the very first run and doesn't get stuck at 33%."]},{"cell_type":"markdown","metadata":{"id":"NOVtD23P-dQy"},"source":["The following cells are taken from the lab session, they will also save checkpoints: if you don't want checkpoints please comment the lines  'callbacks.append(ckpt_callback)' in each cell."]},{"cell_type":"code","metadata":{"id":"pwLv1ujOCnP4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605895310734,"user_tz":-60,"elapsed":4350563,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"cc6e461e-2f28-4c4e-9167-5c7576dbf6df"},"source":["import os\n","from datetime import datetime\n","\n","cwd = cwd  # use your local directory if you are not using Drive\n","\n","exps_dir = os.path.join(cwd, 'classification_experiments_')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'Fine_tuning'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True,\n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# Early Stopping\n","\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)\n","\n","# Fit the model (few epochs to check the validity of the model)\n","\n","model.fit(x=train_dataset,\n","          epochs=25,  \n","          steps_per_epoch=len(train_gen),\n","          validation_data=valid_dataset,\n","          validation_steps=len(valid_gen), \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","162/162 [==============================] - 2792s 17s/step - loss: 1.5573 - accuracy: 0.5474 - val_loss: 0.9804 - val_accuracy: 0.7232\n","Epoch 2/25\n","162/162 [==============================] - 305s 2s/step - loss: 0.9687 - accuracy: 0.7338 - val_loss: 0.7987 - val_accuracy: 0.8348\n","Epoch 3/25\n","162/162 [==============================] - 300s 2s/step - loss: 0.8667 - accuracy: 0.7795 - val_loss: 0.8064 - val_accuracy: 0.8125\n","Epoch 4/25\n","162/162 [==============================] - 307s 2s/step - loss: 0.7681 - accuracy: 0.8337 - val_loss: 0.7046 - val_accuracy: 0.8661\n","Epoch 5/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.7182 - accuracy: 0.8436 - val_loss: 0.6964 - val_accuracy: 0.8705\n","Epoch 6/25\n","162/162 [==============================] - 308s 2s/step - loss: 0.6876 - accuracy: 0.8653 - val_loss: 0.6414 - val_accuracy: 0.8839\n","Epoch 7/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.6474 - accuracy: 0.8779 - val_loss: 0.5989 - val_accuracy: 0.9062\n","Epoch 8/25\n","162/162 [==============================] - 304s 2s/step - loss: 0.6087 - accuracy: 0.8844 - val_loss: 0.6427 - val_accuracy: 0.8839\n","Epoch 9/25\n","162/162 [==============================] - 311s 2s/step - loss: 0.5869 - accuracy: 0.8970 - val_loss: 0.5925 - val_accuracy: 0.8906\n","Epoch 10/25\n","162/162 [==============================] - 303s 2s/step - loss: 0.5605 - accuracy: 0.9063 - val_loss: 0.6265 - val_accuracy: 0.8683\n","Epoch 11/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.5281 - accuracy: 0.9160 - val_loss: 0.6456 - val_accuracy: 0.8973\n","Epoch 12/25\n","162/162 [==============================] - 305s 2s/step - loss: 0.5026 - accuracy: 0.9239 - val_loss: 0.5598 - val_accuracy: 0.8996\n","Epoch 13/25\n","162/162 [==============================] - 312s 2s/step - loss: 0.4872 - accuracy: 0.9292 - val_loss: 0.5409 - val_accuracy: 0.9241\n","Epoch 14/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.4652 - accuracy: 0.9326 - val_loss: 0.5535 - val_accuracy: 0.9107\n","Epoch 15/25\n","162/162 [==============================] - 315s 2s/step - loss: 0.4369 - accuracy: 0.9441 - val_loss: 0.5106 - val_accuracy: 0.9219\n","Epoch 16/25\n","162/162 [==============================] - 307s 2s/step - loss: 0.4158 - accuracy: 0.9446 - val_loss: 0.6410 - val_accuracy: 0.8750\n","Epoch 17/25\n","162/162 [==============================] - 316s 2s/step - loss: 0.4088 - accuracy: 0.9458 - val_loss: 0.6011 - val_accuracy: 0.8884\n","Epoch 18/25\n","162/162 [==============================] - 305s 2s/step - loss: 0.4027 - accuracy: 0.9460 - val_loss: 0.5117 - val_accuracy: 0.9219\n","Epoch 19/25\n","162/162 [==============================] - 305s 2s/step - loss: 0.3772 - accuracy: 0.9545 - val_loss: 0.5519 - val_accuracy: 0.8996\n","Epoch 20/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.3679 - accuracy: 0.9530 - val_loss: 0.4975 - val_accuracy: 0.9062\n","Epoch 21/25\n","162/162 [==============================] - 296s 2s/step - loss: 0.3482 - accuracy: 0.9601 - val_loss: 0.5164 - val_accuracy: 0.9085\n","Epoch 22/25\n","162/162 [==============================] - 312s 2s/step - loss: 0.3311 - accuracy: 0.9638 - val_loss: 0.5912 - val_accuracy: 0.9040\n","Epoch 23/25\n","162/162 [==============================] - 305s 2s/step - loss: 0.3028 - accuracy: 0.9696 - val_loss: 0.5732 - val_accuracy: 0.9196\n","Epoch 24/25\n","162/162 [==============================] - 304s 2s/step - loss: 0.3193 - accuracy: 0.9584 - val_loss: 0.5011 - val_accuracy: 0.9174\n","Epoch 25/25\n","162/162 [==============================] - 309s 2s/step - loss: 0.2899 - accuracy: 0.9684 - val_loss: 0.4367 - val_accuracy: 0.9308\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc8e3f3deb8>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"3Vj2x7H46r37"},"source":["We tried 15 more epochs, starting from the best model of the previous 25: validation accuracy is stable around 92%, but the model seems unable to improve. Being unable to further refine our result, we didn't submit new predictions, and we interrupted our trainings."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"9XGkugoxjn30","executionInfo":{"status":"ok","timestamp":1605941536084,"user_tz":-60,"elapsed":4315,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"98bd20e6-194f-41dc-fb10-34a14abef2e2"},"source":["dir_with_weights = '/content/drive/My Drive/ANNDL_Results/Challenge1/Checkpoints/Transfer_model_weights'\n","dir_with_weights\n","\n","model.load_weights(os.path.join(dir_with_weights, 'TransferCheckpoint.ckpt'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/ANNDL_Results/Challenge1/Checkpoints/Transfer_model_weights'"]},"metadata":{"tags":[]},"execution_count":16},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fae895e9588>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MZzQ9YejpjT","executionInfo":{"status":"ok","timestamp":1605945914014,"user_tz":-60,"elapsed":4191978,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"69981d46-93f1-4ad6-a9e3-6acfd449f15a"},"source":["import os\n","from datetime import datetime\n","\n","cwd = cwd  # use your local directory if you are not using Drive\n","\n","exps_dir = os.path.join(cwd, 'classification_experiments_')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'Fine_tuning_2'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True,\n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# Early Stopping\n","\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)\n","\n","# Fit the model\n","\n","model.fit(x=train_dataset,\n","          epochs=15,  \n","          steps_per_epoch=len(train_gen),\n","          validation_data=valid_dataset,\n","          validation_steps=len(valid_gen), \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","162/162 [==============================] - 1183s 7s/step - loss: 0.2708 - accuracy: 0.9743 - val_loss: 0.4637 - val_accuracy: 0.9375\n","Epoch 2/15\n","162/162 [==============================] - 216s 1s/step - loss: 0.2718 - accuracy: 0.9708 - val_loss: 0.4729 - val_accuracy: 0.9196\n","Epoch 3/15\n","162/162 [==============================] - 215s 1s/step - loss: 0.2635 - accuracy: 0.9692 - val_loss: 0.5200 - val_accuracy: 0.9152\n","Epoch 4/15\n","162/162 [==============================] - 214s 1s/step - loss: 0.2508 - accuracy: 0.9739 - val_loss: 0.4934 - val_accuracy: 0.9152\n","Epoch 5/15\n","162/162 [==============================] - 213s 1s/step - loss: 0.2272 - accuracy: 0.9787 - val_loss: 0.4643 - val_accuracy: 0.9286\n","Epoch 6/15\n","162/162 [==============================] - 213s 1s/step - loss: 0.2313 - accuracy: 0.9760 - val_loss: 0.4769 - val_accuracy: 0.9241\n","Epoch 7/15\n","162/162 [==============================] - 212s 1s/step - loss: 0.1987 - accuracy: 0.9851 - val_loss: 0.6257 - val_accuracy: 0.8862\n","Epoch 8/15\n","162/162 [==============================] - 214s 1s/step - loss: 0.2134 - accuracy: 0.9772 - val_loss: 0.4105 - val_accuracy: 0.9353\n","Epoch 9/15\n","162/162 [==============================] - 213s 1s/step - loss: 0.1941 - accuracy: 0.9822 - val_loss: 0.5640 - val_accuracy: 0.8951\n","Epoch 10/15\n","162/162 [==============================] - 211s 1s/step - loss: 0.2007 - accuracy: 0.9774 - val_loss: 0.4380 - val_accuracy: 0.9308\n","Epoch 11/15\n","162/162 [==============================] - 211s 1s/step - loss: 0.1815 - accuracy: 0.9830 - val_loss: 0.5382 - val_accuracy: 0.9107\n","Epoch 12/15\n","162/162 [==============================] - 211s 1s/step - loss: 0.1753 - accuracy: 0.9826 - val_loss: 0.5734 - val_accuracy: 0.8996\n","Epoch 13/15\n","162/162 [==============================] - 212s 1s/step - loss: 0.1733 - accuracy: 0.9810 - val_loss: 0.4005 - val_accuracy: 0.9263\n","Epoch 14/15\n","162/162 [==============================] - 211s 1s/step - loss: 0.1552 - accuracy: 0.9849 - val_loss: 0.4586 - val_accuracy: 0.9062\n","Epoch 15/15\n","162/162 [==============================] - 215s 1s/step - loss: 0.1461 - accuracy: 0.9876 - val_loss: 0.4579 - val_accuracy: 0.9219\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fae80550b38>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"qMT8gkjHLeEZ"},"source":["# Test the model, generate the csv file"]},{"cell_type":"code","metadata":{"id":"HaNzRvLKLkZu"},"source":["# Import necessary libraries\n","\n","import os\n","\n","from datetime import datetime\n","\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFje3EQTNXFb"},"source":["# Given function for saving the csv file, once the experiment is complete\n","\n","def create_csv(results, results_dir='./'):\n","\n","    csv_fname = 'results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cG7u2F0wODMH"},"source":["# our version\n","\n","from keras.applications.vgg16 import preprocess_input\n","\n","test_dir = os.path.join(dataset_dir, 'test')\n","image_filenames = next(os.walk(test_dir))[2]\n","\n","results = {}\n","\n","# Iterate on each image\n","for image_name in image_filenames:\n","\n","   # Open image\n","   img = Image.open(os.path.join(test_dir, image_name)).convert('RGB')\n","\n","   # Create a tensor from each image\n","\n","   img_array = np.array(img.resize((img_w,img_h)))\n","   img_array = np.expand_dims(img_array, 0) \n","\n","   # Normalize, predict and add to the dictionary\n","   softmax = model.predict(x = preprocess_input(img_array))\n","   prediction = tf.argmax(softmax,1)\n","   results[image_name] = int(prediction)\n","\n","# Create csv file with the given function\n","create_csv(results,work_dir3)"],"execution_count":null,"outputs":[]}]}