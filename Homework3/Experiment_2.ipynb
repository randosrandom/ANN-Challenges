{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Experiment_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"_tGw3GYXqT_v"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSb4Ql5PqnI5"},"source":["# Import libraries and fix seed to make experiments reproducible\n","import os\n","import tensorflow as tf\n","import numpy as np\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","cwd = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsAnj9qCqqEV","executionInfo":{"status":"ok","timestamp":1611817178503,"user_tz":-60,"elapsed":27624,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"b2b461f5-ff19-4663-bbc2-c487a42f01ab"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n6orK_0zsl-8"},"source":["# Unzip VQA Dataset\r\n","!unzip '/content/drive/My Drive/ANNDL_Homeworks3/anndl-2020-vqa.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScNJ13-DM-L6","executionInfo":{"status":"ok","timestamp":1611817369091,"user_tz":-60,"elapsed":189316,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"a2c7f30e-6b85-48b5-e828-ae89b373a7ff"},"source":["!ls '/content/VQA_Dataset/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Images\ttest_questions.json  train_questions_annotations.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jLlFe7jVv3DX"},"source":["# My working environment\r\n","env = '/content/drive/My Drive/ANNDL_Homeworks3/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlKHX0t2T5lV"},"source":["# **Import Data from the json file**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cV8jiZ-Px7j0","executionInfo":{"status":"ok","timestamp":1611817396695,"user_tz":-60,"elapsed":937,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"8553f39b-2c3a-4e51-866c-adcf0747f04b"},"source":["# Load data from json file\n","import json \n","\n","f = open('/content/VQA_Dataset/train_questions_annotations.json') \n","_data = json.load(f)\n","\n","questions = []\n","answers = []\n","image_ids = []\n","data_ids = []\n","for data_id in _data:\n","  _id, _answer, _image_id, _question = data_id, _data[data_id]['answer'], _data[data_id]['image_id'], _data[data_id]['question']\n","  data_ids.append(_id)\n","  questions.append(_question)\n","  answers.append(_answer)\n","  image_ids.append(_image_id)\n","\n","print('Number of sentences:', len(questions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of sentences: 58832\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8NVM0cgKWUB4"},"source":["# Create a random validation_list, which is a list of indices, to split data in training and validation\r\n","\r\n","import math, random\r\n","\r\n","list_of_index = list(range(len(questions)))\r\n","val_rate = 0.1\r\n","val_size = (math.ceil(len(questions) * val_rate))\r\n","\r\n","validation_list = random.sample(list_of_index, val_size)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"THwBaICN8YaG"},"source":["# Split data for training and validation using the list just created\n","\n","train_questions = []\n","train_answers = []\n","train_image_ids = []\n","train_data_ids = []\n","\n","validation_questions= []\n","validation_answers= []\n","validation_image_ids = []\n","validation_data_ids = []\n","\n","i = 0\n","\n","for i in list_of_index:\n","  if i in validation_list:\n","    validation_data_ids.append(data_ids[i])\n","    validation_questions.append(questions[i])\n","    validation_answers.append(answers[i])\n","    validation_image_ids.append(image_ids[i])\n","  else:\n","    train_data_ids.append(data_ids[i])\n","    train_questions.append(questions[i])\n","    train_answers.append(answers[i])\n","    train_image_ids.append(image_ids[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvLyXTmoPaZ-"},"source":["# **Data Pre-Processing and Dataset Creation**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWf3JYzkTq43","executionInfo":{"status":"ok","timestamp":1611817414244,"user_tz":-60,"elapsed":1749,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"8de0df77-93de-4ed3-e268-2c927ba9e58d"},"source":["# TOKENIZATION\n","\n","# Convert words to integers\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","questions_tokenizer = Tokenizer()\n","questions_tokenizer.fit_on_texts(questions)\n","questions_tokenized = questions_tokenizer.texts_to_sequences(questions)\n","\n","questions_wtoi = questions_tokenizer.word_index\n","print('Total words used in questions:', len(questions_wtoi))\n","\n","max_questions_length = max(len(sentence) for sentence in questions_tokenized)\n","print('Max questions sentence length:', max_questions_length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total words used in questions: 4640\n","Max questions sentence length: 21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dji4dgMyWS5g","executionInfo":{"status":"ok","timestamp":1611817423105,"user_tz":-60,"elapsed":6098,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"db952ec9-5252-427b-d046-ba7acfddc163"},"source":["# Padding sequences\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","questions_encoder_inputs = pad_sequences(questions_tokenized, maxlen=max_questions_length)\n","print(\"Questions encoder inputs shape:\", questions_encoder_inputs.shape)\n","\n","questions_encoder_inputs_train = []\n","questions_encoder_inputs_validation = []\n","\n","for count in list_of_index:\n","  if count in validation_list:\n","    questions_encoder_inputs_validation.append(questions_encoder_inputs[count])\n","  else:\n","    questions_encoder_inputs_train.append(questions_encoder_inputs[count])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Questions encoder inputs shape: (58832, 21)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IAS9psdBT9hJ"},"source":["# Labels Dictionary\n","\n","labels_dict = {\n","        '0': 0,\n","        '1': 1,\n","        '2': 2,\n","        '3': 3,\n","        '4': 4,\n","        '5': 5,\n","        'apple': 6,\n","        'baseball': 7,\n","        'bench': 8,\n","        'bike': 9,\n","        'bird': 10,\n","        'black': 11,\n","        'blanket': 12,\n","        'blue': 13,\n","        'bone': 14,\n","        'book': 15,\n","        'boy': 16,\n","        'brown': 17,\n","        'cat': 18,\n","        'chair': 19,\n","        'couch': 20,\n","        'dog': 21,\n","        'floor': 22,\n","        'food': 23,\n","        'football': 24,\n","        'girl': 25,\n","        'grass': 26,\n","        'gray': 27,\n","        'green': 28,\n","        'left': 29,\n","        'log': 30,\n","        'man': 31,\n","        'monkey bars': 32,\n","        'no': 33,\n","        'nothing': 34,\n","        'orange': 35,\n","        'pie': 36,\n","        'plant': 37,\n","        'playing': 38,\n","        'red': 39,\n","        'right': 40,\n","        'rug': 41,\n","        'sandbox': 42,\n","        'sitting': 43,\n","        'sleeping': 44,\n","        'soccer': 45,\n","        'squirrel': 46,\n","        'standing': 47,\n","        'stool': 48,\n","        'sunny': 49,\n","        'table': 50,\n","        'tree': 51,\n","        'watermelon': 52,\n","        'white': 53,\n","        'wine': 54,\n","        'woman': 55,\n","        'yellow': 56,\n","        'yes': 57\n","}\n","\n","num_classes = 58"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUyYHLwdqTHg"},"source":["# ImageDataGenerator\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","img_h = 256\n","img_w = 256\n","\n","apply_data_augmentation = True\n","\n","# Create training ImageDataGenerator object\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=2,\n","                                      width_shift_range=2,\n","                                      height_shift_range=2,\n","                                      zoom_range=0.05,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=False,\n","                                      fill_mode='reflect')\n","else:\n","    img_data_gen = ImageDataGenerator(fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owCBZg0kqTKq"},"source":["from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  def __init__(self, which_subset, filenames, img_generator, encoder_input, preprocessing_function, output): \n","\n","    self.which_subset = which_subset\n","    self.subset_filenames = filenames\n","    self.img_generator = img_generator\n","    self.encoder_input = encoder_input\n","    self.preprocessing_function = preprocessing_function\n","    self.output = output\n","\n","  def __len__(self):\n","    return len(self.subset_filenames)\n","\n","  def __getitem__(self, index):\n","    # Read Image and Sentence\n","    curr_filename = self.subset_filenames[index]\n","    img = Image.open(os.path.join(curr_filename))\n","    img = img.convert('RGB')\n","    sentence = self.encoder_input[index]\n","    answer = self.output[index]\n","\n","    # Resize image\n","    img = img.resize([img_h, img_w])\n","    \n","    # Converting in numpy arrays\n","    img_arr = np.array(img)\n","    sentence = np.array(sentence)\n","    answer = np.array(answer)\n","\n","    if self.which_subset == 'training':\n","      if self.img_generator is not None:\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","        \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    # return couples image-question and answers\n","    return (img_arr, sentence), answer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0ev2o7TZnEm"},"source":["# Creating images filenames (for training and validation) which will be used to create the CustomDataset\n","\n","filenames_train = []\n","filenames_validation = []\n","\n","for i in list_of_index:\n","  if i in validation_list:\n","    filenames_validation.append('/content/VQA_Dataset/Images/' + image_ids[i] + '.png')\n","  else:\n","    filenames_train.append('/content/VQA_Dataset/Images/' + image_ids[i] + '.png')\n","\n","filenames_train = np.array(filenames_train)\n","filenames_validation = np.array(filenames_validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D72khuzThmRt"},"source":["# Creating one-hot-encoded arrays for output answers (both training and validation)\r\n","\r\n","output_answers_train = []\r\n","output_answers_validation = []\r\n","\r\n","for ans in train_answers:\r\n","  output_answers_train.append(labels_dict[ans])\r\n","output_answers_train = tf.one_hot(output_answers_train, depth=num_classes)\r\n","\r\n","for ans in validation_answers:\r\n","  output_answers_validation.append(labels_dict[ans])\r\n","output_answers_validation = tf.one_hot(output_answers_validation, depth=num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXoLNxg6qTNk"},"source":["# Creating the custom datasets using homonym class\n","\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","\n","dataset_train = CustomDataset(which_subset='training', filenames=filenames_train, \n","                        img_generator=img_data_gen, encoder_input=questions_encoder_inputs_train,\n","                        preprocessing_function=preprocess_input, output=output_answers_train)\n","\n","dataset_valid = CustomDataset(which_subset='validation', filenames=filenames_validation, \n","                              img_generator=None, encoder_input=questions_encoder_inputs_validation,\n","                              preprocessing_function=preprocess_input, output=output_answers_validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVQiKT6dyaIG","executionInfo":{"status":"ok","timestamp":1611817448723,"user_tz":-60,"elapsed":2572,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"1cc85a9f-131b-40d5-b03a-c43a17ece3e4"},"source":["# Eventually create final datasets for training and validation\n","\n","batch_size = 64\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset_train,\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\n","                                               output_shapes=(([img_h, img_w, 3], [max_questions_length]), [num_classes]))\n","\n","train_dataset.shuffle\n","\n","train_dataset = train_dataset.batch(batch_size=batch_size)\n","\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\n","                                               output_shapes=(([img_h, img_w, 3], [max_questions_length]), [num_classes]))\n","\n","valid_dataset.shuffle\n","\n","valid_dataset = valid_dataset.batch(batch_size=batch_size)\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method DatasetV2.shuffle of <FlatMapDataset shapes: (((256, 256, 3), (21,)), (58,)), types: ((tf.float32, tf.int32), tf.int32)>>"]},"metadata":{"tags":[]},"execution_count":18},{"output_type":"execute_result","data":{"text/plain":["<bound method DatasetV2.shuffle of <FlatMapDataset shapes: (((256, 256, 3), (21,)), (58,)), types: ((tf.float32, tf.int32), tf.int32)>>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"5xOSTWqwvZTI"},"source":["# **Model**"]},{"cell_type":"markdown","metadata":{"id":"TOozNlGRS0-P"},"source":["CNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"linrpaSMvcEc","executionInfo":{"status":"ok","timestamp":1611817451320,"user_tz":-60,"elapsed":1542,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"edd85a74-f006-4990-e446-f661ab0453dc"},"source":["# Image features extraction with VGG16\n","\n","vgg = tf.keras.applications.VGG16(\n","    include_top=False, \n","    weights=\"imagenet\",\n","    input_shape=(img_h, img_w, 3)\n",")\n","\n","finetuning = True\n","\n","if finetuning:\n","    freeze_until = 15\n","    \n","    for layer in vgg.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    vgg.trainable = False\n","\n","CNN = tf.keras.Sequential()\n","CNN.add(vgg)\n","CNN.add(tf.keras.layers.Flatten())\n","CNN.add(tf.keras.layers.Dropout(0.5))\n","CNN.add(tf.keras.layers.Dense(units=128, activation='relu'))\n","\n","input_CNN = tf.keras.Input(shape=(img_h, img_w, 3))\n","out_CNN = CNN(input_CNN)\n","\n","vgg.summary()\n","CNN.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 7,079,424\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 8, 8, 512)         14714688  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 32768)             0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 32768)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               4194432   \n","=================================================================\n","Total params: 18,909,120\n","Trainable params: 11,273,856\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4U4WTDsGTfEZ"},"source":["RNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYS0KBrgXoM9","executionInfo":{"status":"ok","timestamp":1611817493096,"user_tz":-60,"elapsed":2257,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"c5062da4-20f5-4ab3-dec1-c993b17188d1"},"source":["# Encode questions with LSTM\n","\n","EMBEDDING_SIZE = 64\n","\n","encoder_input = tf.keras.Input(shape=[max_questions_length])\n","encoder_embedding_layer = tf.keras.layers.Embedding(len(questions_wtoi)+1, EMBEDDING_SIZE, input_length=max_questions_length, mask_zero=True)\n","encoder_embedding_out = encoder_embedding_layer(encoder_input)\n","encoder = tf.keras.layers.LSTM(units=128)\n","encoder_output = encoder(encoder_embedding_out)\n","\n","encoder_model = tf.keras.Model(encoder_input, encoder_output)\n","out_encoder=encoder_model(encoder_input)\n","\n","encoder_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 21)]              0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 21, 64)            297024    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 128)               98816     \n","=================================================================\n","Total params: 395,840\n","Trainable params: 395,840\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8EEXysRHac6X"},"source":[" Merging CNN and RNN and finalizing the model\r\n"]},{"cell_type":"code","metadata":{"id":"N65IGZmYETq0"},"source":["# Merge\n","merge_model = tf.keras.layers.multiply(inputs=[out_CNN, out_encoder])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNiKcrsdgN43"},"source":["# Adding Dense layer after the merge\r\n","\r\n","merge_model = tf.keras.layers.Dense(units=64, activation='relu')(merge_model)\r\n","merge_model = tf.keras.layers.Dropout(rate=0.5)(merge_model)\r\n","\r\n","out_merge_model = tf.keras.layers.Dense(units=num_classes, activation='softmax')(merge_model)\r\n","\r\n","VQA_model = tf.keras.Model(inputs=[input_CNN, encoder_input], outputs=out_merge_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7SG9zA_lAMX"},"source":["# **Compile and Fit the model**"]},{"cell_type":"code","metadata":{"id":"_y-DPr3EBFvK"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Categorical Crossentropy loss\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Validation metrics\n","# ------------------\n","metrics = 'accuracy'\n","# ------------------\n","\n","# Compile Model\n","VQA_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90So5F3mWbpF"},"source":["import os\r\n","from datetime import datetime\r\n","\r\n","cwd = '/content/drive/My Drive/ANNDL_Homeworks3'\r\n","\r\n","exps_dir = os.path.join(cwd, 'VQA_experiments')\r\n","if not os.path.exists(exps_dir):\r\n","    os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","exp_name = 'VGG16_LSTM'\r\n","\r\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","    os.makedirs(exp_dir)\r\n","    \r\n","callbacks = []\r\n","\r\n","# Model checkpoint\r\n","# ----------------\r\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","if not os.path.exists(ckpt_dir):\r\n","    os.makedirs(ckpt_dir)\r\n","\r\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True,\r\n","                                                   save_weights_only=True)  # False to save the model directly\r\n","callbacks.append(ckpt_callback)\r\n","\r\n","# Early Stopping not needed in this experiment\r\n","\r\n","'''\r\n","early_stop = True\r\n","if early_stop:\r\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\r\n","    callbacks.append(es_callback)\r\n","'''    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUt6zWbQUyZN","executionInfo":{"status":"ok","timestamp":1611770840430,"user_tz":-60,"elapsed":6531089,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"f01c026f-a8fd-4584-df30-98b72ae1c8e3"},"source":["# Fit the model, few epochs at time (see the summary for furher details) \r\n","\r\n","VQA_model.fit(x=train_dataset,\r\n","          epochs=8,\r\n","          steps_per_epoch=len(dataset_train) // batch_size ,\r\n","          validation_data= valid_dataset,\r\n","          validation_steps=len(dataset_valid) // batch_size,\r\n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/8\n","827/827 [==============================] - 1599s 2s/step - loss: 2.7618 - accuracy: 0.2896 - val_loss: 1.5287 - val_accuracy: 0.4811\n","Epoch 2/8\n","827/827 [==============================] - 1568s 2s/step - loss: 1.7003 - accuracy: 0.4313 - val_loss: 1.3192 - val_accuracy: 0.5443\n","Epoch 3/8\n","827/827 [==============================] - 1548s 2s/step - loss: 1.4749 - accuracy: 0.4909 - val_loss: 1.1835 - val_accuracy: 0.5731\n","Epoch 4/8\n","827/827 [==============================] - 1545s 2s/step - loss: 1.3273 - accuracy: 0.5263 - val_loss: 1.1164 - val_accuracy: 0.5857\n","Epoch 5/8\n","827/827 [==============================] - 1565s 2s/step - loss: 1.2330 - accuracy: 0.5526 - val_loss: 1.0756 - val_accuracy: 0.5919\n","Epoch 6/8\n","827/827 [==============================] - 1564s 2s/step - loss: 1.1477 - accuracy: 0.5766 - val_loss: 1.0446 - val_accuracy: 0.5929\n","Epoch 7/8\n","827/827 [==============================] - 1567s 2s/step - loss: 1.0755 - accuracy: 0.5938 - val_loss: 1.0302 - val_accuracy: 0.6029\n","Epoch 8/8\n","827/827 [==============================] - 1563s 2s/step - loss: 1.0166 - accuracy: 0.6154 - val_loss: 1.0458 - val_accuracy: 0.6027\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1f26a9acf8>"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"NI6M6Izk3-QS"},"source":["Now, before beginning a new training we decrease first the learnig rate trying to prenvent model overfitting; then we load the best model (using checkpoint from epoch 7) and we re-fit the model for other 8 epochs."]},{"cell_type":"code","metadata":{"id":"5wOLygB-q-y3"},"source":["# Change learning rate\r\n","\r\n","# Optimization params\r\n","# -------------------\r\n","\r\n","# Loss\r\n","# Categorical Crossentropy loss\r\n","loss = tf.keras.losses.CategoricalCrossentropy()\r\n","# learning rate\r\n","lr = 1e-5\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","# -------------------\r\n","\r\n","# Validation metrics\r\n","# ------------------\r\n","metrics = 'accuracy'\r\n","# ------------------\r\n","\r\n","# Compile Model\r\n","VQA_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92E6aOzyrQjv"},"source":["import os\r\n","from datetime import datetime\r\n","\r\n","cwd = '/content/drive/My Drive/ANNDL_Homeworks3'\r\n","\r\n","exps_dir = os.path.join(cwd, 'VQA_experiments')\r\n","if not os.path.exists(exps_dir):\r\n","    os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","exp_name = 'VGG16_LSTM'\r\n","\r\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","    os.makedirs(exp_dir)\r\n","    \r\n","callbacks = []\r\n","\r\n","# Model checkpoint\r\n","# ----------------\r\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","if not os.path.exists(ckpt_dir):\r\n","    os.makedirs(ckpt_dir)\r\n","\r\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), save_best_only=True,\r\n","                                                   save_weights_only=True)  # False to save the model directly\r\n","callbacks.append(ckpt_callback)\r\n","\r\n","# Early Stopping not needed in this experiment\r\n","\r\n","'''\r\n","early_stop = True\r\n","if early_stop:\r\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\r\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDx5Vzf5rS_T","executionInfo":{"status":"ok","timestamp":1611817528808,"user_tz":-60,"elapsed":6726,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"e3298a52-e130-4e8e-dfa6-70dbe23b5882"},"source":["# load best checkpoint from the 7th epoch of the first training\r\n","\r\n","VQA_model.load_weights(os.path.join(env, 'VQA_experiments', 'VGG16_LSTM_Jan27_14-38-30', 'ckpts', 'cp_07.ckpt'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa903f08048>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"z20C-NBosGNg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611829982985,"user_tz":-60,"elapsed":942875,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"57a913f2-4bf9-484e-a247-f9fc3e2f22b8"},"source":["# Re-fit the model\r\n","\r\n","VQA_model.fit(x=train_dataset,\r\n","          epochs=8, \r\n","          steps_per_epoch=len(dataset_train) // batch_size ,\r\n","          validation_data= valid_dataset,\r\n","          validation_steps=len(dataset_valid) // batch_size,\r\n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/8\n","827/827 [==============================] - 1598s 2s/step - loss: 1.0364 - accuracy: 0.6080 - val_loss: 0.9147 - val_accuracy: 0.6437\n","Epoch 2/8\n","827/827 [==============================] - 1549s 2s/step - loss: 0.9672 - accuracy: 0.6317 - val_loss: 0.9256 - val_accuracy: 0.6346\n","Epoch 3/8\n","827/827 [==============================] - 1545s 2s/step - loss: 0.9123 - accuracy: 0.6446 - val_loss: 0.9341 - val_accuracy: 0.6387\n","Epoch 4/8\n","827/827 [==============================] - 1547s 2s/step - loss: 0.8668 - accuracy: 0.6634 - val_loss: 0.9905 - val_accuracy: 0.6350\n","Epoch 5/8\n","827/827 [==============================] - 1551s 2s/step - loss: 0.8084 - accuracy: 0.6862 - val_loss: 1.0183 - val_accuracy: 0.6314\n","Epoch 6/8\n","827/827 [==============================] - 1549s 2s/step - loss: 0.7711 - accuracy: 0.6965 - val_loss: 1.0475 - val_accuracy: 0.6253\n","Epoch 7/8\n","827/827 [==============================] - 1546s 2s/step - loss: 0.7140 - accuracy: 0.7198 - val_loss: 1.0943 - val_accuracy: 0.6245\n","Epoch 8/8\n","827/827 [==============================] - 1553s 2s/step - loss: 0.6776 - accuracy: 0.7320 - val_loss: 1.1233 - val_accuracy: 0.6219\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fa903f10eb8>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"ukxdOeVSjljS"},"source":["# **Generate csv file for predictions**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f97vyXOzcUYi","executionInfo":{"status":"ok","timestamp":1611140748816,"user_tz":-60,"elapsed":9726,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"7825339a-7a7a-487b-8297-558a2496379a"},"source":["# load best checkpoint to generate predictions, using 1st checkpoint of the second training\r\n","\r\n","VQA_model.load_weights(os.path.join(env, 'VQA_experiments', 'VGG16_LSTM_Jan28_07-05-16', 'ckpts', 'cp_01.ckpt'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5f65b10710>"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"f1dsAVjCj6y_"},"source":["# Import necessary libraries\r\n","\r\n","import os\r\n","\r\n","from datetime import datetime\r\n","\r\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbcJJ-1gd5VP"},"source":["# Given function for saving the csv file, once the experiment is complete\r\n","\r\n","def create_csv(results, results_dir='./'):\r\n","\r\n","    csv_fname = 'results_'\r\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\r\n","\r\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\r\n","\r\n","        f.write('Id,Category\\n')\r\n","\r\n","        for key, value in results.items():\r\n","            f.write(key + ',' + str(value) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oi0tbo-YtcYy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611139113291,"user_tz":-60,"elapsed":1017,"user":{"displayName":"randeep singh","photoUrl":"","userId":"05852941244413530926"}},"outputId":"4c1efba5-bc88-4fe9-a914-67a2654b0593"},"source":["# Import data from Test_questions.json\r\n","import json \r\n","\r\n","f = open('/content/VQA_Dataset/test_questions.json') \r\n","_data = json.load(f)\r\n","\r\n","questions_test = []\r\n","image_ids_test = []\r\n","data_ids_test = []\r\n","for data_id in _data:\r\n","  _id, _image_id, _question = data_id, _data[data_id]['image_id'], _data[data_id]['question']\r\n","  data_ids_test.append(_id)\r\n","  questions_test.append(_question)\r\n","  image_ids_test.append(_image_id + '.png')\r\n","\r\n","print('Number of sentences:', len(questions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of sentences: 58832\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2qiWO_zuXLH"},"source":["# Creation of the csv file\r\n","\r\n","from tensorflow.keras.applications.vgg16 import preprocess_input \r\n","results ={}\r\n","\r\n","for i in range(len(questions_test)):\r\n","  \r\n","  # Open image and convert to RGB\r\n","  img = Image.open(os.path.join('/content/VQA_Dataset/Images', image_ids_test[i])).convert('RGB')\r\n","\r\n","  # Create a tensor from each image and preprocess with vgg preprocessing function\r\n","  img_arr = np.array(img.resize([img_h, img_w]))\r\n","  img_arr = np.expand_dims(img_arr,0)\r\n","  img_arr = preprocess_input(img_arr)\r\n","\r\n","  # Tokenize the question and convert to numpy array\r\n","  input_tokenized = questions_tokenizer.texts_to_sequences([questions_test[i]])\r\n","  input_tokenized = pad_sequences(input_tokenized, maxlen = max_questions_length)\r\n","  quest_arr = np.array(input_tokenized)\r\n","\r\n","  # Input for the model\r\n","  input = (img_arr, quest_arr)\r\n","\r\n","  # Predict and add to the dictionary\r\n","  softmax = VQA_model.predict(input)\r\n","  prediction = tf.argmax(softmax,1)\r\n","  results[data_ids_test[i]] = int(prediction)\r\n","\r\n","  # Close opened image\r\n","  img.close()\r\n","\r\n","#Create csv file for prediction\r\n","create_csv(results, env)"],"execution_count":null,"outputs":[]}]}