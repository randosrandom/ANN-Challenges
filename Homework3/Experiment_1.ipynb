{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Experiment_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"_tGw3GYXqT_v"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSb4Ql5PqnI5"},"source":["# Import libraries and fix seed to make experiments reproducible\n","import os\n","import tensorflow as tf\n","import numpy as np\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","cwd = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsAnj9qCqqEV","executionInfo":{"status":"ok","timestamp":1611668286654,"user_tz":-60,"elapsed":22237,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"b96fa6c7-3a3a-4943-8884-269225b6ac37"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n6orK_0zsl-8"},"source":["# Unzip vqa dataset\n","!unzip '/content/drive/MyDrive/Challenge3/anndl-2020-vqa.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScNJ13-DM-L6","executionInfo":{"status":"ok","timestamp":1611668547818,"user_tz":-60,"elapsed":562,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"7da011b8-3a5f-4c34-c5c6-728b54cd7747"},"source":["!ls '/content/VQA_Dataset/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Images\ttest_questions.json  train_questions_annotations.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jLlFe7jVv3DX"},"source":["# My working environment\n","env = '/content/drive/My Drive/Challenge3'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlKHX0t2T5lV"},"source":["# **Import Data from the json file**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cV8jiZ-Px7j0","executionInfo":{"status":"ok","timestamp":1611668559323,"user_tz":-60,"elapsed":784,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"415712f2-74c1-4426-ef57-0aa0787bb38c"},"source":["# Load data from json file\n","import json \n","\n","f = open('/content/VQA_Dataset/train_questions_annotations.json') \n","_data = json.load(f)\n","\n","questions = []\n","answers = []\n","image_ids = []\n","data_ids = []\n","for data_id in _data:\n","  _id, _answer, _image_id, _question = data_id, _data[data_id]['answer'], _data[data_id]['image_id'], _data[data_id]['question']\n","  data_ids.append(_id)\n","  questions.append(_question)\n","  answers.append(_answer)\n","  image_ids.append(_image_id)\n","\n","print('Number of sentences:', len(questions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of sentences: 58832\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8NVM0cgKWUB4"},"source":["# Create a random validation_list, which is a list of indices, to split data in training and validation\n","\n","import math, random\n","\n","list_of_index = list(range(len(questions)))\n","val_rate = 0.1\n","val_size = (math.ceil(len(questions) * val_rate))\n","\n","validation_list = random.sample(list_of_index, val_size)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"THwBaICN8YaG"},"source":["# Split data for training and validation using the list just created\n","\n","train_questions = []\n","train_answers = []\n","train_image_ids = []\n","train_data_ids = []\n","\n","validation_questions= []\n","validation_answers= []\n","validation_image_ids = []\n","validation_data_ids = []\n","\n","i = 0\n","\n","for i in list_of_index:\n","  if i in validation_list:\n","    validation_data_ids.append(data_ids[i])\n","    validation_questions.append(questions[i])\n","    validation_answers.append(answers[i])\n","    validation_image_ids.append(image_ids[i])\n","  else:\n","    train_data_ids.append(data_ids[i])\n","    train_questions.append(questions[i])\n","    train_answers.append(answers[i])\n","    train_image_ids.append(image_ids[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvLyXTmoPaZ-"},"source":["# **Data Pre-Processing and Dataset Creation**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWf3JYzkTq43","executionInfo":{"status":"ok","timestamp":1611668573852,"user_tz":-60,"elapsed":1547,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"b2de17a2-b541-49d8-c3c4-c5d34c9333b2"},"source":["# TOKENIZATION\n","\n","# Convert words to integers\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","questions_tokenizer = Tokenizer()\n","questions_tokenizer.fit_on_texts(questions)\n","questions_tokenized = questions_tokenizer.texts_to_sequences(questions)\n","\n","questions_wtoi = questions_tokenizer.word_index\n","print('Total words used in questions:', len(questions_wtoi))\n","\n","max_questions_length = max(len(sentence) for sentence in questions_tokenized)\n","print('Max questions sentence length:', max_questions_length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total words used in questions: 4640\n","Max questions sentence length: 21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dji4dgMyWS5g","executionInfo":{"status":"ok","timestamp":1611668581283,"user_tz":-60,"elapsed":5313,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"d3a459b8-ae43-427d-b9d3-cfb57856f3ef"},"source":["# Padding sequences\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","questions_encoder_inputs = pad_sequences(questions_tokenized, maxlen=max_questions_length)\n","print(\"Questions encoder inputs shape:\", questions_encoder_inputs.shape)\n","\n","questions_encoder_inputs_train = []\n","questions_encoder_inputs_validation = []\n","\n","for count in list_of_index:\n","  if count in validation_list:\n","    questions_encoder_inputs_validation.append(questions_encoder_inputs[count])\n","  else:\n","    questions_encoder_inputs_train.append(questions_encoder_inputs[count])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Questions encoder inputs shape: (58832, 21)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IAS9psdBT9hJ"},"source":["# Labels Dictionary\n","\n","labels_dict = {\n","        '0': 0,\n","        '1': 1,\n","        '2': 2,\n","        '3': 3,\n","        '4': 4,\n","        '5': 5,\n","        'apple': 6,\n","        'baseball': 7,\n","        'bench': 8,\n","        'bike': 9,\n","        'bird': 10,\n","        'black': 11,\n","        'blanket': 12,\n","        'blue': 13,\n","        'bone': 14,\n","        'book': 15,\n","        'boy': 16,\n","        'brown': 17,\n","        'cat': 18,\n","        'chair': 19,\n","        'couch': 20,\n","        'dog': 21,\n","        'floor': 22,\n","        'food': 23,\n","        'football': 24,\n","        'girl': 25,\n","        'grass': 26,\n","        'gray': 27,\n","        'green': 28,\n","        'left': 29,\n","        'log': 30,\n","        'man': 31,\n","        'monkey bars': 32,\n","        'no': 33,\n","        'nothing': 34,\n","        'orange': 35,\n","        'pie': 36,\n","        'plant': 37,\n","        'playing': 38,\n","        'red': 39,\n","        'right': 40,\n","        'rug': 41,\n","        'sandbox': 42,\n","        'sitting': 43,\n","        'sleeping': 44,\n","        'soccer': 45,\n","        'squirrel': 46,\n","        'standing': 47,\n","        'stool': 48,\n","        'sunny': 49,\n","        'table': 50,\n","        'tree': 51,\n","        'watermelon': 52,\n","        'white': 53,\n","        'wine': 54,\n","        'woman': 55,\n","        'yellow': 56,\n","        'yes': 57\n","}\n","\n","num_classes = 58"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUyYHLwdqTHg"},"source":["# ImageDataGenerator\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","img_h = 256\n","img_w = 256\n","\n","apply_data_augmentation = False\n","\n","# Create training ImageDataGenerator object\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=10,\n","                                      width_shift_range=10,\n","                                      height_shift_range=10,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect')\n","else:\n","    img_data_gen = ImageDataGenerator(fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owCBZg0kqTKq"},"source":["from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","\n","  def __init__(self, which_subset, filenames, img_generator, encoder_input, preprocessing_function, output): \n","\n","    self.which_subset = which_subset\n","    self.subset_filenames = filenames\n","    self.img_generator = img_generator\n","    self.encoder_input = encoder_input\n","    self.preprocessing_function = preprocessing_function\n","    self.output = output\n","\n","  def __len__(self):\n","    return len(self.subset_filenames)\n","\n","  def __getitem__(self, index):\n","    \n","    # Read image and sentence\n","    curr_filename = self.subset_filenames[index]\n","    img = Image.open(os.path.join(curr_filename))\n","    img = img.convert('RGB')\n","    sentence = self.encoder_input[index]\n","    answer = self.output[index]\n","\n","    # Resize image\n","    img = img.resize([img_h, img_w])\n","    \n","    # Converting in numpy arrays\n","    img_arr = np.array(img)\n","    sentence = np.array(sentence)\n","    answer = np.array(answer)\n","\n","    if self.which_subset == 'training':\n","      if self.img_generator is not None:\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","        \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    # return couples image-question and answers\n","    return (img_arr, sentence), answer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0ev2o7TZnEm"},"source":["# Creating images filenames (for training and validation) which will be used to create the CustomDataset\n","\n","filenames_train = []\n","filenames_validation = []\n","\n","for i in list_of_index:\n","  if i in validation_list:\n","    filenames_validation.append('/content/VQA_Dataset/Images/' + image_ids[i] + '.png')\n","  else:\n","    filenames_train.append('/content/VQA_Dataset/Images/' + image_ids[i] + '.png')\n","\n","filenames_train = np.array(filenames_train)\n","filenames_validation = np.array(filenames_validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D72khuzThmRt"},"source":["# Creating one-hot-encoded arrays for output answers (both training and validation)\n","\n","output_answers_train = []\n","output_answers_validation = []\n","\n","for ans in train_answers:\n","  output_answers_train.append(labels_dict[ans])\n","output_answers_train = tf.one_hot(output_answers_train, depth=num_classes)\n","\n","for ans in validation_answers:\n","  output_answers_validation.append(labels_dict[ans])\n","output_answers_validation = tf.one_hot(output_answers_validation, depth=num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXoLNxg6qTNk"},"source":["# Creating the custom datasets using homonym class\n","\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","\n","dataset_train = CustomDataset(which_subset='training', filenames=filenames_train, \n","                        img_generator=img_data_gen, encoder_input=questions_encoder_inputs_train,\n","                        preprocessing_function=preprocess_input, output=output_answers_train)\n","\n","dataset_valid = CustomDataset(which_subset='validation', filenames=filenames_validation, \n","                              img_generator=None, encoder_input=questions_encoder_inputs_validation,\n","                              preprocessing_function=preprocess_input, output=output_answers_validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVQiKT6dyaIG","executionInfo":{"status":"ok","timestamp":1611668626054,"user_tz":-60,"elapsed":548,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"b5b465e9-4741-49f5-e204-d4d47ff35834"},"source":["# Eventually create final datasets for training and validation\n","\n","batch_size=64\n","\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset_train,\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\n","                                               output_shapes=(([img_h, img_w, 3], [max_questions_length]), [num_classes]))\n","\n","train_dataset.shuffle\n","\n","train_dataset = train_dataset.batch(batch_size=batch_size)\n","\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\n","                                               output_shapes=(([img_h, img_w, 3], [max_questions_length]), [num_classes]))\n","\n","valid_dataset.shuffle\n","\n","valid_dataset = valid_dataset.batch(batch_size=batch_size)\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method DatasetV2.shuffle of <FlatMapDataset shapes: (((256, 256, 3), (21,)), (58,)), types: ((tf.float32, tf.int32), tf.int32)>>"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"execute_result","data":{"text/plain":["<bound method DatasetV2.shuffle of <FlatMapDataset shapes: (((256, 256, 3), (21,)), (58,)), types: ((tf.float32, tf.int32), tf.int32)>>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"5xOSTWqwvZTI"},"source":["# **Model**"]},{"cell_type":"markdown","metadata":{"id":"TOozNlGRS0-P"},"source":["CNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"linrpaSMvcEc","executionInfo":{"status":"ok","timestamp":1611668632251,"user_tz":-60,"elapsed":2062,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"c9eb76fd-decc-4760-a3a1-4e4ba109d31e"},"source":["# Image features extraction with VGG16\n","\n","vgg = tf.keras.applications.VGG16(\n","    include_top=False, \n","    weights=\"imagenet\",\n","    input_shape=(img_h, img_w, 3)\n",")\n","\n","finetuning = False\n","\n","if finetuning:\n","    freeze_until = 15\n","    \n","    for layer in vgg.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    vgg.trainable = False\n","\n","CNN = tf.keras.Sequential()\n","CNN.add(vgg)\n","CNN.add(tf.keras.layers.Flatten())\n","CNN.add(tf.keras.layers.Dropout(0.5))\n","CNN.add(tf.keras.layers.Dense(units=128, activation='relu'))\n","\n","input_CNN = tf.keras.Input(shape=(img_h, img_w, 3))\n","out_CNN = CNN(input_CNN)\n","\n","vgg.summary()\n","CNN.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 8, 8, 512)         14714688  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 32768)             0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 32768)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               4194432   \n","=================================================================\n","Total params: 18,909,120\n","Trainable params: 4,194,432\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4U4WTDsGTfEZ"},"source":["RNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYS0KBrgXoM9","executionInfo":{"status":"ok","timestamp":1611668638335,"user_tz":-60,"elapsed":2030,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"26ef50ae-8061-49dc-a608-e386cf1d0d94"},"source":["# Encode questions with LSTM\n","\n","EMBEDDING_SIZE = 64\n","\n","encoder_input = tf.keras.Input(shape=[max_questions_length])\n","encoder_embedding_layer = tf.keras.layers.Embedding(len(questions_wtoi)+1, EMBEDDING_SIZE, input_length=max_questions_length, mask_zero=True)\n","encoder_embedding_out = encoder_embedding_layer(encoder_input)\n","encoder = tf.keras.layers.LSTM(units=128)\n","encoder_output = encoder(encoder_embedding_out)\n","\n","encoder_model = tf.keras.Model(encoder_input, encoder_output)\n","out_encoder=encoder_model(encoder_input)\n","\n","encoder_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 21)]              0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 21, 64)            297024    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 128)               98816     \n","=================================================================\n","Total params: 395,840\n","Trainable params: 395,840\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8EEXysRHac6X"},"source":[" Merging CNN and RNN and finalizing the model\n"]},{"cell_type":"code","metadata":{"id":"N65IGZmYETq0"},"source":["# Merge\n","\n","merge_model = tf.keras.layers.multiply(inputs=[out_CNN, out_encoder])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNiKcrsdgN43"},"source":["# Adding dense layer and the final softmax after the merge\n","\n","merge_model = tf.keras.layers.Dense(units=64, activation='tanh')(merge_model)\n","merge_model = tf.keras.layers.Dropout(rate=0.5)(merge_model)\n","\n","out_merge_model = tf.keras.layers.Dense(units=num_classes, activation='softmax')(merge_model)\n","\n","VQA_model = tf.keras.Model(inputs=[input_CNN, encoder_input], outputs=out_merge_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7SG9zA_lAMX"},"source":["# **Compile and Fit the model**"]},{"cell_type":"code","metadata":{"id":"_y-DPr3EBFvK"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Categorical Crossentropy loss\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Validation metrics\n","# ------------------\n","metrics = 'accuracy'\n","# ------------------\n","\n","# Compile Model\n","VQA_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90So5F3mWbpF"},"source":["import os\n","from datetime import datetime\n","\n","cwd = '/content/drive/My Drive/Challenge3/'\n","\n","exps_dir = os.path.join(cwd, 'VQA_experiments')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'VGG16_LSTM'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'),\n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# Early Stopping\n","\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUt6zWbQUyZN","executionInfo":{"status":"ok","timestamp":1611681904413,"user_tz":-60,"elapsed":13248826,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"b68f48ae-da45-4380-d0a0-a021aee2fc55"},"source":["# Fit the model\n","\n","VQA_model.fit(x=train_dataset,\n","          epochs=50,  #### set repeat in training dataset\n","          steps_per_epoch=len(dataset_train) // batch_size ,\n","          validation_data= valid_dataset,\n","          validation_steps=len(dataset_valid) // batch_size,\n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","827/827 [==============================] - 1042s 1s/step - loss: 2.6045 - accuracy: 0.3250 - val_loss: 1.4738 - val_accuracy: 0.5055\n","Epoch 2/50\n","827/827 [==============================] - 1003s 1s/step - loss: 1.5415 - accuracy: 0.4735 - val_loss: 1.2639 - val_accuracy: 0.5561\n","Epoch 3/50\n","827/827 [==============================] - 1000s 1s/step - loss: 1.3167 - accuracy: 0.5425 - val_loss: 1.1632 - val_accuracy: 0.5776\n","Epoch 4/50\n","827/827 [==============================] - 1002s 1s/step - loss: 1.1702 - accuracy: 0.5845 - val_loss: 1.1081 - val_accuracy: 0.5886\n","Epoch 5/50\n","827/827 [==============================] - 1002s 1s/step - loss: 1.0690 - accuracy: 0.6197 - val_loss: 1.0681 - val_accuracy: 0.5951\n","Epoch 6/50\n","827/827 [==============================] - 1002s 1s/step - loss: 0.9729 - accuracy: 0.6561 - val_loss: 1.0470 - val_accuracy: 0.6073\n","Epoch 7/50\n","827/827 [==============================] - 1011s 1s/step - loss: 0.8983 - accuracy: 0.6848 - val_loss: 1.0389 - val_accuracy: 0.6013\n","Epoch 8/50\n","827/827 [==============================] - 1031s 1s/step - loss: 0.8300 - accuracy: 0.7080 - val_loss: 1.0338 - val_accuracy: 0.6087\n","Epoch 9/50\n","827/827 [==============================] - 1021s 1s/step - loss: 0.7706 - accuracy: 0.7305 - val_loss: 1.0359 - val_accuracy: 0.6078\n","Epoch 10/50\n","827/827 [==============================] - 1018s 1s/step - loss: 0.7234 - accuracy: 0.7500 - val_loss: 1.0362 - val_accuracy: 0.6120\n","Epoch 11/50\n","827/827 [==============================] - 1021s 1s/step - loss: 0.6664 - accuracy: 0.7690 - val_loss: 1.0516 - val_accuracy: 0.6054\n","Epoch 12/50\n","827/827 [==============================] - 1039s 1s/step - loss: 0.6269 - accuracy: 0.7892 - val_loss: 1.0626 - val_accuracy: 0.6051\n","Epoch 13/50\n","827/827 [==============================] - 1049s 1s/step - loss: 0.5854 - accuracy: 0.8032 - val_loss: 1.0775 - val_accuracy: 0.6032\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f4db2dbfa58>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"ukxdOeVSjljS"},"source":["# **Generate csv file for predictions**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f97vyXOzcUYi","executionInfo":{"status":"ok","timestamp":1611685343606,"user_tz":-60,"elapsed":848,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"cbddc92e-3562-4693-d4e1-bc49a070db82"},"source":["# load best checkpoint to generate predictions, which is from the 10th epoch checkpoint of the training above\n","\n","VQA_model.load_weights('/content/drive/MyDrive/Challenge3/VQA_experiments/VGG16_LSTM_Jan26_13-44-13/ckpts/cp_10.ckpt')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4dbd5693c8>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"f1dsAVjCj6y_"},"source":["# Import necessary libraries\n","\n","import os\n","\n","from datetime import datetime\n","\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbcJJ-1gd5VP"},"source":["#Â Given function for saving the csv file, once the experiment is complete\n","\n","def create_csv(results, results_dir='./'):\n","\n","    csv_fname = 'results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oi0tbo-YtcYy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611685353683,"user_tz":-60,"elapsed":604,"user":{"displayName":"Roberto Valendino","photoUrl":"","userId":"13437786348033243579"}},"outputId":"3e5f0e4d-289c-4af3-b733-30113fb6e27c"},"source":["# Import data from Test_questions.json\n","\n","import json \n","\n","f = open('/content/VQA_Dataset/test_questions.json') \n","_data = json.load(f)\n","\n","questions_test = []\n","image_ids_test = []\n","data_ids_test = []\n","for data_id in _data:\n","  _id, _image_id, _question = data_id, _data[data_id]['image_id'], _data[data_id]['question']\n","  data_ids_test.append(_id)\n","  questions_test.append(_question)\n","  image_ids_test.append(_image_id + '.png')\n","\n","print('Number of sentences:', len(questions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of sentences: 58832\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2qiWO_zuXLH"},"source":["# Creation of the csv file\n","\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","results ={}\n","\n","for i in range(len(questions_test)):\n","  \n","  # Open image and convert to RGB\n","  img = Image.open(os.path.join('/content/VQA_Dataset/Images', image_ids_test[i])).convert('RGB')\n","\n","  # Create a tensor from each image and preprocess with vgg preprocessing function\n","  img_arr = np.array(img.resize([img_h, img_w]))\n","  img_arr = np.expand_dims(img_arr,0)\n","  img_arr = preprocess_input(img_arr)\n","\n","  # Tokenize the question and convert to numpy array\n","  input_tokenized = questions_tokenizer.texts_to_sequences([questions_test[i]])\n","  input_tokenized = pad_sequences(input_tokenized, maxlen = max_questions_length)\n","  quest_arr = np.array(input_tokenized)\n","\n","  # Input for the model\n","  input = (img_arr, quest_arr)\n","\n","  # Predict and add to the dictionary\n","  softmax = VQA_model.predict(input)\n","  prediction = tf.argmax(softmax,1)\n","  results[data_ids_test[i]] = int(prediction)\n","\n","  # Close opened image\n","  img.close()\n","\n","# Eventually create csv file for prediction using the function declared before\n","create_csv(results, env)"],"execution_count":null,"outputs":[]}]}